name: Run Notebook and Save Results

on:
  workflow_dispatch: # Enables manual trigger with input parameters
    inputs:
      file_url:
        description: "URL to download the file"
        required: true
        default: "https://example.com/file.csv"
      feature:
        description: "Feature to use (options: tdidf, bow)"
        required: true
        default: "tdidf"
      minGrams:
        description: "Minimum n-gram size"
        required: true
        default: 2
      maxGrams:
        description: "Maximum n-gram size"
        required: true
        default: 4
      maxFeatures:
        description: "Maximum feature size"
        required: true
        default: 250

jobs:
  run-notebook:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas scikit-learn notebook

    - name: Download file
      run: |
        curl -o data.csv -L "${{ github.event.inputs.file_url }}"

    - name: Run notebook
      run: |
        python -m pip install nbconvert
        jupyter nbconvert \
          --to notebook \
          --execute tdIdf.ipynb \
          --output processed_notebook.ipynb \
          --ExecutePreprocessor.extra_arguments="--feature=${{ github.event.inputs.feature }} --minGrams=${{ github.event.inputs.minGrams }} --maxFeatures=${{ github.event.inputs.maxFeatures }} --maxGrams=${{ github.event.inputs.maxGrams }}"

    - name: Archive results
      run: |
        mkdir -p results
        mv ${{ github.event.inputs.feature }}.csv results/
        echo "Results saved to 'results/' directory"
      
    - name: Upload results
      uses: actions/upload-artifact@v3
      with:
        name: processed-results
        path: results/